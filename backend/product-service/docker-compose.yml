version: "3.9"

services:
#  mongo:
#    image: mongo:7
#    container_name: mongo
#    restart: always
#    ports:
#      - "27017:27017"
#    healthcheck:
#      test: [ "CMD", "mongosh", "--eval", "db.adminCommand('ping')" ]
#      interval: 10s
#      timeout: 5s
#      retries: 5
#    volumes:
#      - mongo_data:/data/db
#      - ./mongo_backups:/backups
#    networks:
#      - product-net
#
#  mongo-backup:
#    image: mongo:7
#    container_name: mongo-backup
#    depends_on:
#      - mongo
#    volumes:
#      - ./mongo_backups:/backup
#      - product-service_mongo_data:/data/db
#    entrypoint: >
#      bash -c "while true; do
#        echo 'üîÅ Running MongoDB backup...';
#        mongodump --host mongo --out /backup/$(date +%F_%H-%M-%S);
#        find /backup -type d -mtime +7 -exec rm -rf {} \;
#        sleep 86400;
#      done"
#    networks:
#      - product-net

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.3
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q '\"status\":\"green\"\\|\"status\":\"yellow\"'"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - es_data:/usr/share/elasticsearch/data
    networks:
      - product-net

  product-service:
    build: .
    container_name: product-service
    depends_on:
      - redis
      - elasticsearch
      - logstash
      - category-service
      - kafka

    environment:
      - CATEGORY_SERVICE_URL=http://category-service:8090/api/v1/category
      - MONGO_URI=mongodb://host.docker.internal:27017/product_db
      - ELASTIC_URI=http://elasticsearch:9200
      - JWT_SECRET=supersecretkeythatshouldbereplacedandstoredsecurely
      - SERVER_PORT=8080
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOGSTASH_HOST=logstash
      - LOGSTASH_PORT=5000
    extra_hosts:
      - "host.docker.internal:172.17.0.1"
    ports:
      - "8080:8080"
    networks:
      - product-net

  category-service:
    build: ../category-service
    container_name: category-service
    depends_on:
      - redis
      - elasticsearch
      - logstash
    environment:
      - PRODUCT_SERVICE_URL=http://product-service:8080/api/v1/product
      - MONGO_URI=mongodb://host.docker.internal:27017/category_db
      - ELASTIC_URI=http://elasticsearch:9200
      - JWT_SECRET=supersecretkeythatshouldbereplacedandstoredsecurely
      - SERVER_PORT=8090
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOGSTASH_HOST=logstash
      - LOGSTASH_PORT=5000
    extra_hosts:
      - "host.docker.internal:172.17.0.1"
    ports:
      - "8090:8090"
    networks:
      - product-net

  inventory-service:
    build: ../inventory-service
    container_name: inventory-service
    environment:
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/inventory_db
      - SPRING_DATASOURCE_USERNAME=postgres
      - SPRING_DATASOURCE_PASSWORD=postgres
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - JWT_SECRET=supersecretkeythatshouldbereplacedandstoredsecurely
      - SERVER_PORT=8091
    ports:
      - "8091:8091"
    depends_on:
      - postgres
      - redis
      - kafka
      - logstash
      - product-service
    networks:
      - product-net

  order-service:
    build: ../order-service
    container_name: order-service
    environment:
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/order_db
      - SPRING_DATASOURCE_USERNAME=postgres
      - SPRING_DATASOURCE_PASSWORD=postgres
      - KAFKA_BOOTSTRAP=kafka:9092
      - JWT_SECRET=supersecretkeythatshouldbereplacedandstoredsecurely
      - SERVER_PORT=8092
    ports:
      - "8092:8092"
    depends_on:
      - redis
      - kafka
      - postgres
      - logstash
    networks:
      - product-net

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - product-net

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - product-net

#  postgres:
#    image: postgres:15
#    container_name: postgres
#    environment:
#      POSTGRES_USER: postgres
#      POSTGRES_PASSWORD: postgres
#      POSTGRES_DB: inventory_db
#    ports:
#      - "5432:5432"
#    volumes:
#      - ./init-order-db.sql:/docker-entrypoint-initdb.d/init-order-db.sql
#      - pgdata:/var/lib/postgresql/data
#    networks:
#      - product-net

  redis:
    image: redis:7
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - product-net

  kibana:
    image: docker.elastic.co/kibana/kibana:8.15.3
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - SERVER_NAME=kibana
      - SERVER_HOST=0.0.0.0
      - XPACK_SECURITY_ENABLED=false
    ports:
      - "5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:5601/api/status | grep '\"state\":\"green\"' || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 40s
    networks:
      - product-net

  logstash:
    image: docker.elastic.co/logstash/logstash:8.5.3
    container_name: logstash
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
    ports:
      - "5000:5000"     # TCP input for app logs
      - "9600:9600"     # Logstash monitoring API
    depends_on:
      - elasticsearch
    networks:
      - product-net


  # ---------- Prometheus ----------
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--web.enable-lifecycle'  # allows reload via API
    networks:
      - product-net

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    user: "472" # avoid permission issues; optional
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/dashboards:/var/lib/grafana/dashboards:ro
      - ./monitoring/provisioning:/etc/grafana/provisioning:ro
    networks:
      - product-net

    # ---------- Kafka Exporter (broker metrics & consumer lag) ----------
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: kafka-exporter
    environment:
      - KAFKA_BROKERS=kafka:9092
      - KAFKA_ZOOKEEPER=zookeeper:2181
      # optional: specify topics regex or other envs per image docs
    ports:
      - "9308:9308"   # exporter metrics port
    depends_on:
      - kafka
    networks:
      - product-net

    # optional: filebeat alternative if you prefer shipping log files instead of TCP
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.10.0
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs:/usr/share/filebeat/logs:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - logstash
      - elasticsearch
    networks:
      - product-net

networks:
  product-net:
    driver: bridge

volumes:
  es_data:
  redis_data:
  pgdata:
  grafana_data:
